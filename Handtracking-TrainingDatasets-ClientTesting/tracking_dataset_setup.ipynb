{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a241f90e",
   "metadata": {},
   "source": [
    "# RECORD ACTION TO TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77325a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "recording = False\n",
    "record_data = []\n",
    "start_time = None\n",
    "\n",
    "def normalize_hand(positions):\n",
    "    \"\"\"Chuẩn hóa bàn tay cho static gesture:\n",
    "       - Gốc = cổ tay (landmark 0)\n",
    "       - Scale = khoảng cách cổ tay -> đầu ngón giữa (0 -> 12)\n",
    "       - Bỏ z, chỉ lấy (x, y)\n",
    "    \"\"\"\n",
    "    wrist = np.array(positions[0][:2])        \n",
    "    middle_tip = np.array(positions[12][:2])  \n",
    "\n",
    "    scale = np.linalg.norm(middle_tip - wrist)\n",
    "    if scale < 1e-6:  \n",
    "        scale = 1.0\n",
    "\n",
    "    norm_positions = [((x - wrist[0]) / scale,\n",
    "                       (y - wrist[1]) / scale) for (x, y, z) in positions]\n",
    "    return norm_positions\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb)\n",
    "\n",
    "    positions = None\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            h, w, _ = frame.shape\n",
    "            positions = []\n",
    "            for lm in hand_landmarks.landmark:  \n",
    "                positions.append((lm.x, lm.y, lm.z))  \n",
    "\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                cv2.circle(frame, (cx, cy), 5, (0, 255, 0), -1)\n",
    "\n",
    "    if recording and positions is not None:\n",
    "        norm_positions = normalize_hand(positions)  \n",
    "        record_data.append(norm_positions)\n",
    "\n",
    "        if time.time() - start_time >= 0.3:\n",
    "            filename = f\"hand_record_{int(time.time()*1000)}.npy\"\n",
    "            np.save(filename, np.array(record_data))\n",
    "            print(f\"Saved: {filename}, shape={np.array(record_data).shape}\")\n",
    "            record_data = []\n",
    "            recording = False\n",
    "\n",
    "    cv2.putText(frame, \"Press '1' to record 1s hand motion\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Hand Tracking\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 27:  # ESC to exit\n",
    "        break\n",
    "    elif key == ord('1') and not recording:\n",
    "        print(\"Start recording...\")\n",
    "        recording = True\n",
    "        start_time = time.time()\n",
    "        record_data = []\n",
    "\n",
    "cap.release()\n",
    "hands.close()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d17266",
   "metadata": {},
   "source": [
    "# Rename file, in the correct folder to create category for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1fbaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = r\"D:\\Github\\Machine-Learning-Studies\\Handtracking\\command\\swiperight\"  # PICK YOUR FOLDER\n",
    "files = [f for f in os.listdir(folder_path) if f.endswith(\".npy\")]\n",
    "\n",
    "files.sort()\n",
    "\n",
    "for idx, filename in enumerate(files, 1):\n",
    "    old_path = os.path.join(folder_path, filename)\n",
    "    new_name = f\"swiperight{idx}.npy\" # CHANGE \"commandname\" TO YOUR COMMAND\n",
    "    new_path = os.path.join(folder_path, new_name)\n",
    "    os.rename(old_path, new_path)\n",
    "\n",
    "print(f\"Renamed {len(files)} files to commandname{{x}}.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eeeae2",
   "metadata": {},
   "source": [
    "## Categorize, resize and shaped the datasets for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0f6950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from scipy.interpolate import interp1d\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def load_all_sequences(base_folder):\n",
    "    class_names = sorted(os.listdir(base_folder))\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        class_folder = os.path.join(base_folder, class_name)\n",
    "        if not os.path.isdir(class_folder):\n",
    "            continue\n",
    "        \n",
    "        files = [f for f in os.listdir(class_folder) if f.endswith(\".npy\")]\n",
    "        \n",
    "        for f in files:\n",
    "            seq_path = os.path.join(class_folder, f)\n",
    "            seq = np.load(seq_path)  # shape: (frames, features)\n",
    "            sequences.append(seq)\n",
    "            labels.append(idx)  # gán nhãn bằng index của class\n",
    "    \n",
    "    return np.array(sequences, dtype=object), np.array(labels), class_names\n",
    "\n",
    "\n",
    "folder_path = r\"D:\\Github\\Machine-Learning-Studies\\Handtracking\\command\" # PICK YOUR FOLDER\n",
    "X, y, class_names = load_all_sequences(folder_path)\n",
    "\n",
    "def load_all_sequences(base_folder, balance=False, method=\"undersample\"):\n",
    "    class_names = sorted(os.listdir(base_folder))\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    class_data = {} \n",
    "    \n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        class_folder = os.path.join(base_folder, class_name)\n",
    "        if not os.path.isdir(class_folder):\n",
    "            continue\n",
    "        \n",
    "        files = [f for f in os.listdir(class_folder) if f.endswith(\".npy\")]\n",
    "        class_samples = []\n",
    "        \n",
    "        for f in files:\n",
    "            seq_path = os.path.join(class_folder, f)\n",
    "            seq = np.load(seq_path)  \n",
    "            class_samples.append(seq)\n",
    "        \n",
    "        class_data[idx] = class_samples\n",
    "\n",
    "    if balance:\n",
    "        class_sizes = [len(v) for v in class_data.values()]\n",
    "        if method == \"undersample\":\n",
    "            target_size = min(class_sizes)\n",
    "        elif method == \"oversample\":\n",
    "            target_size = max(class_sizes)\n",
    "        else:\n",
    "            raise ValueError(\"'undersample' or 'oversample'\")\n",
    "        \n",
    "        balanced_sequences = []\n",
    "        balanced_labels = []\n",
    "        \n",
    "        for idx, samples in class_data.items():\n",
    "            if method == \"undersample\":\n",
    "                selected = resample(samples, replace=False, n_samples=target_size, random_state=42)\n",
    "            else:  # oversample\n",
    "                selected = resample(samples, replace=True, n_samples=target_size, random_state=42)\n",
    "            \n",
    "            balanced_sequences.extend(selected)\n",
    "            balanced_labels.extend([idx] * target_size)\n",
    "        \n",
    "        return np.array(balanced_sequences, dtype=object), np.array(balanced_labels), class_names\n",
    "    \n",
    "    for idx, samples in class_data.items():\n",
    "        sequences.extend(samples)\n",
    "        labels.extend([idx] * len(samples))\n",
    "    \n",
    "    return np.array(sequences, dtype=object), np.array(labels), class_names\n",
    "\n",
    "def resize_frames(seq, target_len=12):\n",
    "    \"\"\"\n",
    "    Resize 1 sequence (frames, joints, coords) thành target_len frame.\n",
    "    seq shape: (T, J, C)\n",
    "    \"\"\"\n",
    "    old_len = seq.shape[0]\n",
    "\n",
    "    if old_len == 1:\n",
    "        return np.repeat(seq, target_len, axis=0)\n",
    "\n",
    "    x_old = np.linspace(0, 1, old_len)\n",
    "    x_new = np.linspace(0, 1, target_len)\n",
    "\n",
    "    f = interp1d(x_old, seq, axis=0)\n",
    "    return f(x_new)\n",
    "\n",
    "X_resized = np.array([resize_frames(seq, 10) for seq in X_bal])\n",
    "\n",
    "X_gru = X_resized.reshape(X_resized.shape[0], 10, -1) \n",
    "y_categorical = to_categorical(y_bal, num_classes=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8e97f0",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74414b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "106/106 [==============================] - 7s 18ms/step - loss: 0.6603 - accuracy: 0.7444 - val_loss: 0.4358 - val_accuracy: 0.8057\n",
      "Epoch 2/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3485 - accuracy: 0.8716 - val_loss: 0.2402 - val_accuracy: 0.9100\n",
      "Epoch 3/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.2390 - accuracy: 0.9049 - val_loss: 0.2148 - val_accuracy: 0.9052\n",
      "Epoch 4/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.1654 - accuracy: 0.9382 - val_loss: 0.1108 - val_accuracy: 0.9526\n",
      "Epoch 5/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.1404 - accuracy: 0.9536 - val_loss: 0.2944 - val_accuracy: 0.8957\n",
      "Epoch 6/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.1760 - accuracy: 0.9334 - val_loss: 0.0901 - val_accuracy: 0.9621\n",
      "Epoch 7/50\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.0978 - accuracy: 0.9596 - val_loss: 0.1194 - val_accuracy: 0.9479\n",
      "Epoch 8/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0981 - accuracy: 0.9548 - val_loss: 0.1061 - val_accuracy: 0.9668\n",
      "Epoch 9/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0679 - accuracy: 0.9715 - val_loss: 0.0732 - val_accuracy: 0.9810\n",
      "Epoch 10/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0584 - accuracy: 0.9786 - val_loss: 0.0838 - val_accuracy: 0.9716\n",
      "Epoch 11/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0940 - accuracy: 0.9679 - val_loss: 0.0606 - val_accuracy: 0.9905\n",
      "Epoch 12/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0498 - accuracy: 0.9798 - val_loss: 0.0636 - val_accuracy: 0.9905\n",
      "Epoch 13/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0658 - accuracy: 0.9798 - val_loss: 0.0577 - val_accuracy: 0.9953\n",
      "Epoch 14/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0263 - accuracy: 0.9905 - val_loss: 0.0713 - val_accuracy: 0.9858\n",
      "Epoch 15/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0482 - accuracy: 0.9845 - val_loss: 0.0617 - val_accuracy: 0.9905\n",
      "Epoch 16/50\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.0348 - accuracy: 0.9917 - val_loss: 0.0628 - val_accuracy: 0.9953\n",
      "Epoch 17/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0172 - accuracy: 0.9917 - val_loss: 0.1042 - val_accuracy: 0.9763\n",
      "Epoch 18/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0351 - accuracy: 0.9857 - val_loss: 0.0633 - val_accuracy: 0.9953\n",
      "Epoch 19/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0145 - accuracy: 0.9964 - val_loss: 0.0626 - val_accuracy: 0.9953\n",
      "Epoch 20/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0105 - accuracy: 0.9988 - val_loss: 0.1019 - val_accuracy: 0.9810\n",
      "Epoch 21/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0753 - accuracy: 0.9750 - val_loss: 0.0685 - val_accuracy: 0.9953\n",
      "Epoch 22/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0181 - accuracy: 0.9952 - val_loss: 0.0722 - val_accuracy: 0.9858\n",
      "Epoch 23/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0079 - accuracy: 0.9988 - val_loss: 0.0758 - val_accuracy: 0.9858\n",
      "Epoch 24/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0647 - val_accuracy: 0.9953\n",
      "Epoch 25/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0049 - accuracy: 0.9988 - val_loss: 0.0647 - val_accuracy: 0.9953\n",
      "Epoch 26/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0639 - val_accuracy: 0.9953\n",
      "Epoch 27/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0033 - accuracy: 0.9988 - val_loss: 0.0660 - val_accuracy: 0.9953\n",
      "Epoch 28/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0651 - val_accuracy: 0.9953\n",
      "Epoch 29/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 8.0694e-04 - accuracy: 1.0000 - val_loss: 0.0660 - val_accuracy: 0.9953\n",
      "Epoch 30/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 9.2251e-04 - accuracy: 1.0000 - val_loss: 0.0670 - val_accuracy: 0.9953\n",
      "Epoch 31/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 7.9106e-04 - accuracy: 1.0000 - val_loss: 0.0659 - val_accuracy: 0.9953\n",
      "Epoch 32/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0670 - val_accuracy: 0.9953\n",
      "Epoch 33/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 4.9463e-04 - accuracy: 1.0000 - val_loss: 0.0682 - val_accuracy: 0.9953\n",
      "Epoch 34/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0854 - accuracy: 0.9822 - val_loss: 0.2758 - val_accuracy: 0.9242\n",
      "Epoch 35/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.2101 - accuracy: 0.9382 - val_loss: 0.0974 - val_accuracy: 0.9763\n",
      "Epoch 36/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0753 - accuracy: 0.9798 - val_loss: 0.0615 - val_accuracy: 0.9953\n",
      "Epoch 37/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0154 - accuracy: 0.9941 - val_loss: 0.0615 - val_accuracy: 0.9953\n",
      "Epoch 38/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0117 - accuracy: 0.9964 - val_loss: 0.0616 - val_accuracy: 0.9953\n",
      "Epoch 39/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0072 - accuracy: 0.9976 - val_loss: 0.0635 - val_accuracy: 0.9953\n",
      "Epoch 40/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0060 - accuracy: 0.9976 - val_loss: 0.0692 - val_accuracy: 0.9953\n",
      "Epoch 41/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0555 - accuracy: 0.9774 - val_loss: 0.0836 - val_accuracy: 0.9953\n",
      "Epoch 42/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.0782 - val_accuracy: 0.9953\n",
      "Epoch 43/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0064 - accuracy: 0.9988 - val_loss: 0.0813 - val_accuracy: 0.9953\n",
      "Epoch 44/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0802 - val_accuracy: 0.9953\n",
      "Epoch 45/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0781 - val_accuracy: 0.9953\n",
      "Epoch 46/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0811 - val_accuracy: 0.9953\n",
      "Epoch 47/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0770 - val_accuracy: 0.9953\n",
      "Epoch 48/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0764 - val_accuracy: 0.9953\n",
      "Epoch 49/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 5.7044e-04 - accuracy: 1.0000 - val_loss: 0.0777 - val_accuracy: 0.9953\n",
      "Epoch 50/50\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0777 - val_accuracy: 0.9953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2302b489c00>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Bidirectional\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_gru, y_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Sequential([\n",
    "    Bidirectional(GRU(128, return_sequences=True), input_shape=(X_gru.shape[1], X_gru.shape[2])),\n",
    "    Bidirectional(GRU(64)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(y_categorical.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34091a48",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562a614b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Github\\Machine-Learning-Studies\\tf-env\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"action_gru_model.h5\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8545f60",
   "metadata": {},
   "source": [
    "# Test the model on camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45deba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import load_model\n",
    "from collections import deque\n",
    "import time\n",
    "import os\n",
    "\n",
    "# ========= CONFIG =========\n",
    "MODEL_PATH = \"action_gru_model.h5\"\n",
    "\n",
    "ACTIONS = ['activate', 'nocommand', 'swipeleft', 'swiperight'] # RENAME TO YOUR COMMANDS\n",
    "\n",
    "USE_Z = False   # IF YOU WANT TO USE Z-AXIS, SET TO TRUE\n",
    "\n",
    "# ========= LOAD MODEL =========\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "# Input shape: (None, T, F)\n",
    "TIMESTEPS = model.input_shape[1]\n",
    "FEATURES = model.input_shape[2]\n",
    "print(f\"[INFO] Model input shape: T={TIMESTEPS}, F={FEATURES}\")\n",
    "\n",
    "# ========= HANDS INIT =========\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# ========= FEATURE EXTRACT =========\n",
    "def normalize_positions(landmarks, use_z=False):\n",
    "    \"\"\"Chuẩn hóa tọa độ bàn tay:\n",
    "       - Lấy cổ tay (id=0) làm gốc\n",
    "       - Scale theo khoảng cách cổ tay -> đầu ngón giữa (id=12)\n",
    "       - Trả về vector (42,) hoặc (63,)\n",
    "    \"\"\"\n",
    "    wrist = np.array([landmarks[0].x, landmarks[0].y] + ([landmarks[0].z] if use_z else []))\n",
    "    middle_tip = np.array([landmarks[12].x, landmarks[12].y] + ([landmarks[12].z] if use_z else []))\n",
    "\n",
    "    scale = np.linalg.norm(middle_tip - wrist)\n",
    "    if scale < 1e-6:\n",
    "        scale = 1.0\n",
    "\n",
    "    feats = []\n",
    "    for lm in landmarks:\n",
    "        vec = np.array([lm.x, lm.y] + ([lm.z] if use_z else []))\n",
    "        norm = (vec - wrist) / scale\n",
    "        feats.extend(norm.tolist())\n",
    "    return np.array(feats, dtype=np.float32)\n",
    "\n",
    "\n",
    "def extract_features(results):\n",
    "    if results.multi_hand_landmarks:\n",
    "        hand = results.multi_hand_landmarks[0]\n",
    "        return normalize_positions(hand.landmark, use_z=USE_Z)\n",
    "    return np.zeros((FEATURES,), dtype=np.float32)\n",
    "\n",
    "\n",
    "# ========= SEQ BUFFER =========\n",
    "seq_buffer = deque(maxlen=TIMESTEPS)\n",
    "\n",
    "# ========= CAMERA =========\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# ========= LOOP =========\n",
    "fps_time = time.time()\n",
    "last_pred = None\n",
    "last_prob = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    feats = extract_features(results)\n",
    "    seq_buffer.append(feats)\n",
    "\n",
    "    if len(seq_buffer) == TIMESTEPS:\n",
    "        seq_input = np.expand_dims(seq_buffer, axis=0)  # (1, T, F)\n",
    "        preds = model.predict(seq_input, verbose=0)[0]\n",
    "        max_idx = np.argmax(preds)\n",
    "        last_pred = ACTIONS[max_idx] if max_idx < len(ACTIONS) else str(max_idx)\n",
    "        last_prob = preds[max_idx]\n",
    "\n",
    "    # ===== DISPLAY =====\n",
    "    fps = 1.0 / (time.time() - fps_time)\n",
    "    fps_time = time.time()\n",
    "\n",
    "    cv2.putText(frame, f\"Pred: {last_pred} ({last_prob:.2f})\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"FPS: {fps:.1f}\", (10, 70),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Hand Gesture Recognition\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC to EXIT\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-env)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
